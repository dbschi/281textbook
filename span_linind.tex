\section{Span, Linear Independence}
\subsection{Span}
Throughout this section, let $\mathbb{K}$ be a field. We constrain ourselves to work in $V$, a $\mathbb{K}$-vector space.
\definition{Span}{
	Let $k$ be some positive integer. Let  $\{v_1, v_2, ... , v_k\}\subseteq V$. The \textbf{span} of $\{v_1, v_2, ... , v_k\}$ is denoted as \[
	\textrm{span}(v_1,v_2,...,v_k)
	\]
	and is the \textit{smallest} subspace of $V$ containing $\{v_1, v_2, ... , v_k\}$.
}
The \textit{smallest} here means that if another subspace $W$ contains $\{v_1,...,v_k\}$, $W$ cannot be a subset of $\textrm{span}(v_1,...,v_k)$. How do we know such a subspace exists? We can take the intersection of all the subspaces containing $\{v_1,...,v_k\}$ \[
\textrm{span}(v_1,...,v_k) = \bigcap_{W \textrm{ subspace containing } \{v_1,...,v_k\}} W
\]
which is a subspace containing $\{v_1,...,v_k\}$ and is a subset of all other subspaces containing $\{v_1,...,v_k\}$. We know $V\subseteq V$ is a subspace containing $\{v_1,...,v_k\}$, so the intersection is between at least one set and is thus well-defined.
\proposition{
     The span of $\{v_1, v_2,\ ... , v_k\}\subseteq V$ is 
    all linear combinations of the vectors in the set:
    \[\textrm{span}\{v_1, v_2,\ ... , v_k\} = \{c_1v_1 + c_2v_2 + ... + c_kv_k | c_1, c_2, ... c_k \in \mathbb{K}\}\]
}
\begin{proof}
	We first show $\textrm{span}\{v_1, v_2, ... , v_k\} \supseteq \{c_1v_1 + c_2v_2 + ... + c_kv_k | c_1, c_2, ... c_k \in \mathbb{K}\}$. That is, for every $W$ subset containing $v_1,...,v_k$, $W$ must also contain $c_1v_1+...+c_kv_k$. \\
	We now show $\textrm{span}\{v_1, v_2, ... , v_k\} \subseteq \{c_1v_1 + c_2v_2 + ... + c_kv_k | c_1, c_2, ... c_k \in \mathbb{K}\}$. The right side is a set that nonempty, is closed under addition and scalar multiplication, and contains $v_1= 1v_1+0v_2+...+0v_k$,...,$v_k=0v_1+...+0v_{k-1}+1v_k$. Therefore, it is one of the $W$ subspaces whose intersection is used to construct the span.
\end{proof}

% insert examples including what the span of one, two, etc. vectors is

It can be difficult to how to think about what the span of a set of vectors looks like, though it is also important to develop
an intuition for it as more complex techniques are developed. It is also important to consider what vectors span a given subspace.

\example{
	Sketch, in $\reals^3$, the following spans: span$(\{\vec{i}\})$, span$(\{\vec{i},\vec{j}\})$, span$(\{\vec{i},\vec{j},\vec{k}\})$.
}
\todo write something here\\
\subsection{Linear Independence}
Thinking geometrically in the previous example, the span of one vector is one-dimensional (a line), the span of two vectors is two-dimensional (a plane),
and the span of three vectors is three-dimensional (the whole 3D space). Specifically,
we can give a correspondence between the linear combination of $k$ vectors $\{c_1{v}_1,...,c_k{v}_k\}$and a point in $\reals^k$ as \[
	c_1{v}_1+c_2{v}_2+...+c_k{v}_k \sim \begin{bmatrix}
		c_1 \\ c_2\\ ...\\ c_k
	\end{bmatrix}
\]
Assuming this correspondence works, the geomtry of the span should resemble $\reals^k$.
Unfortunately, this is not always true, for instance, consider the following counterexample:
\example{
	Sketch, in $\reals^3$, $\textrm{span}(\vec{i},\vec{k},\vec{i}+2\vec{k})$.
}
\todo

What went wrong here is that the third vector $\vec{i}+2\vec{k}$ is already a linear combination of the first two, and so this vector is `redundant' in the set.
More precisely, $(1,2,0)$ and $(0,0,1)$ give the same linear combination of the three vectors, so the correspondence fails. We thus want to answer the following question:
\begin{quotation}
	Given a set of $k$ vectors $\{{v}_1,...,{v}_k\}$, when do these vectors span the full $k$-dimensions?
	If they do not span the full $k$ dimensions, how many dimensions do they span?  
\end{quotation}

We already have one candidate criterion for spanning the full dimensions.
\definition{Linear Independence}{
	Let ${{v}_1,...,v_k}\subset V$. We say that $v_1,...,v_k$ are \textbf{linearly independent} if every linear combination of $v_1,...,v_k$ is unique. Precisely, if there are $c_1,...,c_k, d_1,...,d_k\in\mathbb{K}$ such that \[
	c_1v_1+c_2v_2+...+c_kv_k = d_1v_1+d_2v_2+...+d_kv_k,
	\]
	then \[c_1=d_1, c_2=d_2,...,c_k=d_k.\]
	If $\{v_1,...,v_k\}$ is not linearly independent, we say that it is \textbf{linearly dependent}.
}
This might be very tricky to verify, so we would like some simplier definitions for linear independence.
\proposition{
	%\iffalse
	The following are equivalent definitions for linear independence:
	\begin{itemize}
		\item (*) There is only one way to make $0_V$. i.e. If $c_1v_1+...+c_kv_k=0_V$, then $c_1=c_2=...=c_k=0$.
		\item (**) If $k\geq2$, there is no way to express one vector as a linear combination as the others. i.e. For all $1\leq j\leq k$, $v_j\notin \textrm{span}(v_1,...,v_{j-1},v_{j+1},...,v_k)$.
	\end{itemize}
	%\fi
}
\begin{proof}%[ Linear independent  $\iff$ (*)]
	We first show the original definition of linear independence implies (*).
	We already have one way to create the zero vector as a linear combination of the set, namely\[
	0v_1+0v_2+...+0v_k=0_V.
	\] By definition of linear independence, this is the only way to create the zero vector.
	\\
	We now show (*) implies the original definition of linear independence. Let (*) hold. Then if \[
	c_1v_1+...+c_kv_k=d_1v_1+...+d_kv_k,
	\] then rearranging the terms we will get \[
		(c_1-d_1)v_1+...+(c_k-d_k)v_k = 0_V.
	\]
	Since there is only one way to make $0_V$, it must be that $c_1-d_1=c_2-d_2=...=c_k-d_k=0$, or $c_1=d_1,...c_k=d_k$. This matches our original definition of linear independence.
\\
	Now we set $k\geq 2$ and show the equivelence of (*) and (**).
	We now show (*) implies (**).  
	Let (*) hold, and $v\in\textrm{span}(v_1,...,v_{j-1},v_{j+1},...,v_{k})$.
	Then $v-v_j$ is a linear combination of $v_1,...,v_k$ that is not $0v_1+...+0v_k$ as the coefficient before $v_j$ is $-1$. So that \[
		v-v_j\neq 0_V \implies v\neq v_j.
	\]
	What we have shown is that everything in $\textrm{span}(v_1,...,v_{j-1},v_{j+1},...,v_{k})$ is not $v_j$. Which means $v_j\notin \textrm{span}(v_1,...,v_{j-1},v_{j+1},...,v_k)$.
	\\
	One end of chapter exercise will guide you through the direction (**) implies (*).
\end{proof}
\todo introduce contraposition and contradiction?

We now have an informal statement: \textit{If the set is not linearly independent, then the dimension of the span must be less than k}.
This is because from (**), at least one of the vectors is redundant, so that we can remove that vector and still produce the same span with $k-1$ vectors.
\exercises
\begin{exerciselist}
	\item something here
\end{exerciselist}
\section{Systems of Linear Equations}
As we alluded to earlier, many types of vector spaces are in some way very similar to $\reals^n$. For instance, the span of a set of $k$ linearly independent real-vectors has a natural correspondence to $\reals^k$.
With the blind faith that everything here can generalize nicely back to abstract vector spaces, we limit ourselves again back to talking about $\reals^n$.\\
Here, we introduce a new notation to write linear combiantions, as $c_1v_1+...+c_kv_k$ is very cumbersome.
\definition{Matrix}{
	Let $m$ and $n$ be positive integers. An $m\times n$ \textbf{matrix} is a rectangular array of $m$ rows and $n$ columns in the form \[
		\begin{bmatrix}
			c_{1,1} & c_{1,2} & \ldots & c_{1,n} \\
			c_{2,1} & c_{2,2} & \ldots & c_{2,n} \\
			\vdots & & \ddots  \\
			c_{m,1} & c_{m,2} &\ldots & c_{m,n} 
		\end{bmatrix}\]
	where each $c_{i,j}$ is an \textbf{entry} of the matrix.
	We denote the set of all $m\times n$ matrices with real entries $M_{m\times n}(\reals)$. In general, we have $M_{m\times n} (\mathbb{K})$ for entries in the field $\mathbb{K}$.
}
\begin{notation}
	Suggestively, we can write an $m\times n$ matrix as \[
	\begin{bmatrix}
		\vec{v}_1 & \vec{v}_2 & ... & \vec{v}_n
	\end{bmatrix}
	\]
	where each $\vec{v}_j\in\reals^m$ is written as a column vector\[
	\begin{bmatrix}
		c_{j,1} \\ c_{j,2}\\ \vdots \\ c_{j,m}
	\end{bmatrix}.
	\]
	If we want to think of the matrix by its entries, we can also write the matrix as \[
	\{c_{i,j}\}
	\]
\end{notation}
\definition{Matrix-vector Product}{
	Let \[
	A =\begin{bmatrix}
			\vec{v}_1 & \vec{v}_2 & ... & \vec{v}_n
		\end{bmatrix}
	\] be an $m\times n$ matrix, and \[
	\vec{b}=\begin{bmatrix}
		b_1 \\ b_2 \\ \vdots\\b_n
	\end{bmatrix} \in \reals^n.
	\]
	The product of $A$ and $\vec{b}$ is evaluated as \[
		A\vec{b} = b_1\vec{v}_1+ b_2\vec{v}_2 + ...+ b_n\vec{v}_n.
	\]
}\example{
	Determine if the vectors \[
	\vec{v}_1=\begin{bmatrix}
		1\\0\\1
	\end{bmatrix}
	,\vec{v}_2=\begin{bmatrix}
		1\\1\\0
	\end{bmatrix},
	\vec{v}_3=\begin{bmatrix}
	0\\1\\1
	\end{bmatrix}
	\] are linearly independent.
}
Recalling the definition for linear independence, we solve for 
\[
\begin{bmatrix}
	1&1&0\\
	0& 1 & 1\\
	1& 0 & 1
\end{bmatrix} \begin{bmatrix}
	c_1 \\ c_2\\c_3
\end{bmatrix} = \begin{bmatrix}
	0\\0\\0
\end{bmatrix}.
\]
This is a compact way to express the system
\begin{align*}
	c_1 + c_2 + 0 &= 0 \\
	 0+  c_2 +  c_3 &= 0\\
	c_1+ 0+c_3 &= 0
\end{align*}
There is a very slick way to find the solution (which involves adding all three equations together), but let us solve this methodically. This method is known as elimination, which involves isolating variables, and thus reducing the complexity of the system one by one.
\\
First, we look at the first equation and isolate $c_1=-c_2$. Now we can look replace all instances of $c_1$ in the second and third equation with $-c_2$, giving us \begin{align*}
	c_1 + c_2 + 0 &= 0 \\
	0 +  c_2 +  c_3 &= 0\\
	0 - c_2 +c_3 &= 0
\end{align*}
If we look at just the second and third equations, these only have two variables, so we have effectively decreased the complexity of the system by 1. Whatever we get from the second and third equations, we can substitute back into the first to get $c_1$.
Now, we repeat for the second equation, $c_2=-c_3$, and replacing all instances of $c_2$ we have \begin{align*}
	c_1 + 0 - c_3 &= 0 \\
	0 +  c_2 +  c_3 &= 0\\
	0 + 0 + 2c_3 &= 0
\end{align*}
The third equation now is just an equation in one variable, so we can go ahead and solve \begin{align*}
	c_1 + 0 - c_3 &= 0 \\
	0 +  c_2 +  c_3 &= 0\\
	0 + 0 + c_3 &= 0
\end{align*}
and replace all instances of $c_3$ with $0$ to get 
\begin{align*}
	c_1 + 0 + 0 &= 0 \\
	0 +  c_2+  0 &= 0\\
	0 + 0 + c_3 &= 0
\end{align*}
or $c_1=c_2=c_3=0$. This is indeed a solution to the system, so we can now conclude that these vectors are indeed linearly independent.
If we condense the systems back to matrices (concatenating the $3\times 3$ matrix and the vector on the right)  we get 
\begin{align*}
	\left[
	\begin{array}{ccc|c}
		1&1&0&0\\
		0&1&1&0\\
		1&0&1&0
	\end{array} \right] \rightarrow
	\left[\begin{array}{ccc|c}
		1&1&0&0\\
		0&1&1&0\\
		0&-1&1&0
	\end{array}\right] \rightarrow
	\left[\begin{array}{ccc|c}
		1&0&-1&0\\
		0&1&1&0\\
		0&0&2&0
	\end{array}\right]
	\rightarrow
	\left[\begin{array}{ccc|c}
		1&0&0&0\\
		0&1&0&0\\
		0&0&1&0
	\end{array}\right]
\end{align*}
\example{
	Determine the solutions to the system of equations \begin{align*}
		0 + 2x_2-x_3 &= 1\\
		x_1 - x_2 + x_3 &= 0 \\
		x_1 + x_2 + 2x_3 & = 1
	\end{align*}
}
The first equation here does not let us isolate $x_1$, but we can simply exchange the first two equations to get \[
\begin{bmatrix}[ccc|c]
	1 & -1 & 1 & 0\\
	0&2&-1&1\\
	1& 1 & 2 & 1\\
\end{bmatrix}
\]
We want to remove any dependence of $x_1$ for the other two equations, so we can subtract the first equation from the third to get \[
	\begin{bmatrix}[ccc|c]
		1 & -1 & 1 & 0\\
		0&2&-1&1\\
		0& 2 & 1 & 1\\
	\end{bmatrix}
\]
We divide the second equation by $2$, add this equation to the first, and subtract from the third.
\[
	\begin{bmatrix}[ccc|c]
		1 & -1 & 1 & 0\\
		0&1&\frac{-1}{2}&\frac{1}{2}\\
		0& 2 & 1 & 1\\
	\end{bmatrix} \sim
	\begin{bmatrix}[ccc|c]
		1 & 0 & \frac{1}{2} & \frac{1}{2}\\
		0&1&\frac{-1}{2}&\frac{1}{2}\\
		0& 2 & 1 & 1\\
	\end{bmatrix}
	\sim
	\begin{bmatrix}[ccc|c]
		1 & 0 & \frac{1}{2} & \frac{1}{2}\\
		0&1&\frac{-1}{2}&\frac{1}{2}\\
		0& 0 & 2 & 0\\
	\end{bmatrix}
\]
Finally, we divide the third equation by $2$, and remove all other entries of the third column.
\[
	\begin{bmatrix}[ccc|c]
		1 & 0 & 0& \frac{1}{2}\\
		0&1&0&\frac{1}{2}\\
		0& 0 & 1 & 0\\
	\end{bmatrix}
\]
So the solution is $x_1=1/2, x_2=1/2, x_3=0$.
\subsection{Elementary Row Operations and Row Echelon Form}
By how we manipulated the equations in the previous examples, we motivate these operations on matrices.

\definition{Elementary Row Operations and row equivalence}{
	The following operations on a matrix $A\in M_{m\times n}$ are known as \textbf{elementary row operations}:\begin{enumerate}
		\item \textit{(Row Swap)} Exchange any two rows.
		\item \textit{(Scaling)} Multiply a row by a \textbf{non-zero} constant.
		\item \textit{(Sum)} Add a multiple of a row to another row.
	\end{enumerate}
	Let $B\in M_{m\times n}$. We say that $A$ is \textbf{row equivalent} to $B$ if $A$ can be transformed into $B$ By
	applying a sequence elementary row operations. We denote this equivalence by $A\sim B$.
}
\begin{remark}
	We can group the big set of $M_{m\times n}(\reals^n)$ into classes, where each class contains matrices that are row equivalent to each other.
\end{remark}
\proposition{
	Elementary row operations are invertible. That is, if $A\sim A'$ after applying an elementary row operation,
	$A'\sim A$ through applying a (possibly different) row operation.
}
\proposition{
	An $m\times n$ matrix can be viewed as a system of $m$ equations in $n-1$ variables using the representation in the previous two examples.
	In this representation, row equivalent matrices have the same solution sets.
}
The proof is not very enlightening. It boils down to checking that each of the three elementary row operations do not change the solution set of the corresponding linear system, so a sequence of them will not change the solution sets.
The key takeaway from this is that we can reduce the complexity of the matrix through elementary row operations. Let us define the `simple'
forms of a matrix you can get through row operations.

\definition{Row Echelon Form and Pivots}{
	A matrix is in \textbf{row echelon form} if \begin{itemize}
		\item Rows with all zero entries are on the bottom.
		\item For each row having non-zero entries, the first non-zero entry is on the right of the first non-zero entry of the row above.
	\end{itemize}
	In row echelon form, the first non-zero entry of a row is called a \textbf{pivot}.
	Colloquially, the non-zero entries form a ``staircase'' like shape.
}
\example{
	The following matrix is in row echelon form:\[
	\begin{bmatrix}
		1 & * & * & * & * \\
		0 & 0 & 2 & * & * \\
		0 & 0 & 0 & 1 & * \\
		0 & 0 & 0 & 0 & 0
	\end{bmatrix}
	\]
	The pivots of this matrix, from the first row, are $1$, $2$ and $1$.
}
In most cases, reducing a matrix into the row echelon form is ``good enough'' to find solutions. Start from the bottom row, and substitute variables go up.
However, one problem with working with row echelon form is that a matrix is row equivalent to many matrices in row echelon form, so we want some `super' row echelon form
that is unique to each matrix.
\definition{Reduced Row Echelon Form}{
	A matrix is in \textbf{Reduced Row Echelon Form}(rref) if \begin{itemize}
		\item It is in row echelon form.
		\item All pivots are equal to $1$.
		\item Every pivot is the only non-zero entry in its column.
	\end{itemize}
}
\example{
	The following matrix is in reduced row echelon form:\[
	\begin{bmatrix}
		1 & * & 0 & 0 & * \\
		0 & 0 & 1 & 0 & * \\
		0 & 0 & 0 & 1 & * \\
		0 & 0 & 0 & 0 & 0
	\end{bmatrix}
	\]
}
By how we defined the reduced row echelon form, we cannot find two distinct matrices in reduced row echelon form that are also row equivalent. This guarantees
that a matrix is row equivalent to at most one reduced row echelon form matrix.
\theorem{Existence and Uniqueness of Reduced Row Echelon Form}{
	Let $A \in M_{m\times n}(\reals)$. Then there exists a unique $A'\in M_{m\times n}(\reals)$ such that $A'$ is in reduced
	row echelon form and $A \sim A'$. We define $A'$ to be the \textbf{reduced row echelon form} of $A$, and denote
	\[
		A' = \textrm{rref}(A).
	\]
}

\begin{proof}
	Formalizing our steps in the previous examples, we have an algorithm for reducing a matrix into rref.

	\begin{algorithm}
		\caption{Gaussian-Jordan Reduction of $A\in M_{m\times n}(\reals^n)$}
		
			$i\gets 1$, $j\gets 1$
			\While {$j\leq n$}{
				\For {$k$ in $i\ldots m$} {
					\Comment*[l]{Find a row with non-zero entry in that column}
					\If {$a_{k,j}\neq 0$}{
						 swap(row $i$, row $k$) \Comment*[r]{Move row $k$ to the top row}
						 row $i \gets 1/a{i,j}$ row $i$ \Comment*[r]{Normalize pivot to $1$}
						\For {$l$ in $1\ldots m$, $l\neq i$}{
							 row $l \gets$ row $l$ - $a_{l,j}$ row $i$ \Comment*[r]{all other entries in the same column $=0$}
						}
						$i\gets i+1$ \Comment*[r]{Next row with pivot goes to the row below}
						Break
					}
				}
				 $j\gets j+1$ \Comment*[r]{check pivot in next column}
			}
	\end{algorithm}
	\todo Gaussian-Jordan Reduction
\end{proof}
\example{
	Compute \[
	\textrm{rref}\left(\begin{bmatrix}
		something here lol
	\end{bmatrix}\right)
	\]
}
\subsection{Recovering solutions from rref}
%\subsubsection*{No solutions}
%There are no solutions when the system of equations is inconsistent. In rref, this corresponds to creating a row that says $0=1$.
\theorem{Solutions to linear systems}{
	Let $A\in M_{m\times n}(\reals), \vec{b}\in\reals^m$, the system of equations described by
	$A\vec{x}=\vec{b}$ has\[
	\begin{cases}
		\textrm{No solutions, if rref}\left(\left[A | \vec{b}\right]\right)\textrm{ has a pivot in the last column,}\\
		\textrm{One solution, if all columns but the last has a pivot,} \\
		\textrm{Infinitely many solutions, else.}
	\end{cases}
	\]

}
\todo proof?

\todo no solutions and one solutions

\subsubsection*{Producing infinitely many solutions}
Now, suppose $A\vec{x}=\vec{b}$ has infinitely many solutions. We want to describe this set of solutions without actually
writing infinitely many vectors. One starting point is to see what properties this set satisfies. One starting point 
would be to guess that the set $\{\vec{x}\in\reals^n | A\vec{x}=\vec{b}\}$ is a subspace of $\reals^n$.
This is generally not true, as every subspace contains $\vec{0}_{\reals^n}$, and since $A\vec{0}_{\reals^n}=\vec{0}_{\reals^m}$,
the only possible way this could be a subspace is when $\vec{b}=\vec{0}_{\reals^m}$.\\

We thus start with the solution set to $A\vec{x}=\vec{0}$.
\definition{Nullspace}{
	Let $A\in M_{m\times n}(\reals)$. The \textbf{nullspace} of the A, denoted $N(A)$, is the set of all solutions $\vec{x}$ 
	for $A\vec{x}=\vec{0}$. i.e. \[
		N(A)=\{\vec{x}\in\reals^n | A\vec{x}=\vec{0}_{\reals^m}\}.
	\]
}
\proposition{
	Let $A\in M_{m\times n}(\reals)$. \[
	N(A)= N (\textrm{rref}(A)).\]
}
\begin{proof}
	\todo
\end{proof}
This allows us to limit the discussion of nullspaces to when $A$ is in rref.

It turns out that the nullspace is a subspace. This is why we tend to like systems of linear equations where the right hand side is all zero.
We call these systems \textbf{homogeneous}, and the solutions to homogeneous linear systems form a subspace.
\theorem{Nullspace is a Subspace}{
	Let $A\in M_{m\times n}(\reals)$. $N(A)$ is a subspace of $\reals^n$.
}
\begin{proof}
	\todo zero is in here, closed under addition and multiplication
\end{proof}
\example{
	Find the solutions to the system represented by 

	\[
		\begin{bmatrix}[c c c c c | c]
			1& 0& 1& 0 & -3 & 0\\
			0& 1& 1& 0 & 4 & 0\\
			0& 0& 0& 1 & -2 & 0
		\end{bmatrix}.
	\]
	Express the answer in the form of the span of a set of linearly independent vectors.
}
Let us start with the last variable $x_5$ and work our way towards $x_1$.
There is no equation that fixes $x_5$, so let $x_5 = s$ for some $s\in\reals$.
Then the last equations fixes $x_4 = 2s$. 

Again, we do not have an equation that fixes $x_3$, so let $x_3=t$ for some $t\in\reals$.
This will force (from the second equation) $x_2= -t-4s$ and (from the first equation) $x_1 = -t+3s$.

Our solution vector $(x_1,x_2,x_3,x_4,x_5)$ thus has the form \[
\begin{bmatrix}
	-t+3s\\ -t-4s\\ t\\ 2s\\ s
\end{bmatrix}= s\begin{bmatrix}
	3 \\ -4 \\ 0 \\ 2 \\ 1
\end{bmatrix} + t \begin{bmatrix}
	-1 \\ -1 \\ 1 \\0 \\0
\end{bmatrix}
\]
This is describes exactly the span of the vectors \[
	\begin{bmatrix}
		3 \\ -4 \\ 0 \\ 2 \\ 1
	\end{bmatrix} ,
	\begin{bmatrix}
		-1 \\ -1 \\ 1 \\0 \\0
	\end{bmatrix}.
\]
To confirm these vectors are linearly independent, notice the third entry of the first vector and the fifth entry of the second vector is $0$.

From this example, we have an algorithm to solve for the nullspace as a span of vectors.
\begin{algorithm}
\caption{Generating the Nullspace of Matrix $A$}

	$A\gets \textrm{rref}(A)$
	 $S\gets$ empty set
	\For{each column $j$ without a pivot} {
		 $\vec{v} \gets \vec{e}_j$
		\For{each row $i$ with a pivot}{
			 Locate pivot of row $i$ in column $k$
			 $\vec{v} \gets \vec{v} - a_{i,j} \vec{e}_k$
		}
		Add $\vec{v}$ to S
	 }
	\Return span$(S)$
\end{algorithm}
\\
This is what we did in the previous example, except written in pseudocode.
The idea is that each column without a pivot denotes a free variable, while each column with a pivot is a fixed by the free variables.
This is what we do in lines $5$ to $8$, after fixing our free variable to be $1$, we subtract from the fixed variables. 

\exercises
\begin{exerciselist}
	\item Determine if the following sets of vectors are linearly dependent or independent. \begin{enumerate}[label=(\alph*)]
		\item 
	\end{enumerate}
	\item Let $\{v_1,...,v_k\}\subset V$. Show that exactly one of the following statements hold: \begin{enumerate}[label=(\alph*)]
		\item For every $v\in \textrm{span}(\{v_1,...,v_k\})$, there is exactly one way to write $v$ as a linear combination of $v_1,...,v_k$.
		\item For every $v\in \textrm{span}(\{v_1,...,v_k\})$, there is more than one way to write $v$ as a linear combination of $v_1,...,v_k$.
	\end{enumerate}
\end{exerciselist}