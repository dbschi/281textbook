	
	\chapter{Taylor's Theorem}
    \setcounter{exercisecounter}{0}

\setcounter{thmcounter}{1}
 \section{Analytic Functions and Taylor Series}
In this section we look to develop a method to represent functions as series. An important application of such is the use of series as solutions to differential equations. 

\definition{Power Series}{
    Let $x_0\in \reals$. A \textbf{power series} cenetered at $x_0$ is in the form \[
        f(x)=\sum_{n=0}^{\infty} a_n(x-x_0)^n.
    \]
}
\begin{remark}
    The convention used for $0^0$ when $x=x_0$ here is $0^0=1$. You can imagine that it is the limit of $(x-x_0)^0$ as $x\to x_0$.
\end{remark}
\begin{remark}
    The series might not converge. In fact, the root test for convergence gives an exact criteria for convergence.
\end{remark}
\definition{Radius of Convergence}{
    Let $f(x)$ be a power series centered at $x_0$. The \textbf{radius of convergence} is the value $R$ such that \[
    \begin{cases}
        f(x) \textrm{ converges, } & \textrm{ if $|x-x_0|<R$},\\
        f(x) \textrm{ diverges, } & \textrm{ if $|x-x_0|>R$}.
    \end{cases}
    \]
}
\definition{Analytic Functions}{
Let $\Omega \subseteq \reals$ be open, and $f:\Omega\to\reals$. We say that $f$ is \textbf{analytic} at $x_0$ if there exists $\epsilon>0$, and a power series representation $p_{x_0}(x) = \sum_n a_n(x-x_0)^n$ such that $p_{x_0}(x)$ converges to $f(x)$ in $B(x_0,\epsilon)$. We say that $f$ is analytic on (a,b) if $f$ is analytic at every point in $(a,b)$.
}
\proposition{
    The set of points on which $f$ is analytic form an open set.
}
Being analytic is one of the strictest properties for a function. Analytic functions are infinitely differentiable (smooth).

\theorem{Analytic Functions are Smooth}{Suppose $\sum a_nx^n$ is a power series representation for some function $f$ with a radius of convergence $R > 0$. Then $f$ is infinitely differentiable on $(-R, R)$. }
The proof requires some analysis knowledge out of scope of the course. The hardest part is to show that you can differentiate under the summation, i.e. \[
\frac{d}{dx}\sum_n {f_n(x)} = \sum_n \frac{d}{dx} f_n(x).
\]
Assuming this, we can get power series representations for $f'(x)$ and so on.
\begin{align*}
    \begin{array}{ccccccc}
    f(x) =&  a_0 &+ a_1(x-x_0) &+ a_2(x-x_0)^2 &+a_3(x-x_0)^3&+a_4(x-x_0)^4& +\ldots \\
    f'(x)=&   &a_1 &+ 2a_2(x-x_0) &+ 3a_3(x-x_0)^2 &+4a_4(x-x_0)^3 &+\ldots \\
    f''(x)=& &   &2a_2 &+ 6a_3(x-x_0) &+ 12a_4(x-x_0)^2&+\ldots
    \end{array}
\end{align*}
Importantly, these have the same radius of convergence as the original function (a root test can confirm this), so we get a closed form for the $k$-th derivative of $f$:
\[f^{(k)}(x) = \sum_{n=k}^{\infty} \frac{n!}{(n-k)!}a_n(x-x_0)^{n-k} \] 

\theorem{Uniqueness of Power Series}{
    Let $f$ be analytic at $x_0$. Then its power series representation $\sum_n a_n (x-x_0)^n$ is unique with coefficients
    \[a_k = \frac{f^{(k)}(x_0)}{k!}.\]}

\begin{proof}
    We take the general form of the $k$-th derivative, and evaluate it at $x=x_0$.
    This gives us \[
    f^{(k)}(x_0)=\sum_{n=k}^{\infty} \frac{n!}{(n-k)!}a_n(x_0-x_0)^{n-k}.
    \]
    On the right hand side, all the terms with $n>k$ will evaluate to $0$. The term with $n=k$ evaluates to $k!a_k$. This means \[
    f^{(k)}(x_0) = k! a_k \implies a_k = \frac{f^{(k)}(x_0)}{k!}.
    \]
    Therefore, if a power series exists, it must be in the form \[
    \sum_n \frac{f^{(n)}(x_0)}{n!} (x-x_0).
    \]
\end{proof}
\definition{Taylor Series}{The \textbf{Taylor expansion} of $f$ centered at $x_o$ is given by
\[f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(x_o)}{n!}(x-x_o)^n.\]}

A function locally equals to its Taylor series (about some point) if and only if it is analytic. We will see that if a function is analytic, it is equal to the Taylor series about a point everywhere the series converges.
\theorem{Uniqueness of Analytic Functions}{
    Let $f,g:(a,b)\to\reals$ be analytic. Suppose $f(x)=g(x)$ on some small ball $B(x_0,\epsilon)$. Then $f=g$ everywhere on $(a,b)$.
}

    Let's consider the function $h(x)=f(x)-g(x)$. This is analytic, as we can just take the difference of the respective coefficients in the power series for $f$ and $g$. We have $h(x)=0$ on $B(x_0,\epsilon)$. Our task now is to show that $h(x)=0$ everywhere, so $f=g$ everywhere.

    The idea now is to start with the power series about $x=x_0$ \[
    \sum_{n} 0 (x-x_0)^n.
    \]
    We can `slide' this $x_0$ across a little bit to $x_1\in B(x_0,\epsilon)$ to get \[
        \sum_{n} 0 (x+x_1-x_0-x_1)^n =\sum_{n} 0 (x-x_1)^n 
    \]
    after binomial expansion of the terms $((x-x_1)+(x_1-x_0))^n$. Since $f$ is analytic at $x_1$, there is another ball centered at $x_1$ where $h=0$. Therefore, we can `slide' our center of the power series from $x_1$ to another $x_2$. If we can slide this to everywhere in $(a,b)$ we will get that the power series representation at every point is $0$. 

    \textit{optional material:} How do we guarantee that we can slide everywhere? This requires another idea from analysis called compactness. In short, the compactness of the interval $[x_0,\tilde{x}]$ or $[\tilde{x},x_0]$ guarantees that we can slide our center of the power series across from $x_0$ to any $\tilde{x}\in (a,b)$ in a finite amount of steps. 

    We will give another way to prove this, as we have introduced Zorn's lemma. Without loss of generality, let $x_0<\tilde{x}\in (a,b)$ Consider $S$, the set of points $x\leq \tilde{x}$ that you can `slide to' from $x_0$ in a finite amount of steps. I claim that every increasing sequence in $S$ is bounded above by some element in $S$.
    Let $x_1\leq x_2\leq x_3 \leq \ldots$ be an increasing sequence in $S$. We take $y=\lim_{n\to\infty} x_n$, then the series is bounded above by $y$. To construct the finite sequence going from $x_0$ to $y$, we see that $f$ is analytic at $y$ thus it has a power series representation centered at $y$ that converges to $f$ for some $B(y,\delta)$. We take $m$ large such that $x_m>y-\delta/4$. If we slide the power series centered at $y$ to be centered at $x_m$, the power series converges to $f$ at least in $B(x_m,3\delta/4)\ni y$. That is, you can recenter the power series from $x_m$ to $y$. Therefore, we take the finite sequence that recenters the power series at $x_0$ to $x_m$, then recenter that sequence at $y$. Therefore $S$ contains a maximal element by Zorn's lemma. Finally, to find out what this maximal element is, we make use of the fact that the points where $f$ is analytic is open. Therefore, the only point that can be the maximal element of $S$ is $\tilde{x}$, which is used as the upper limit of all elements in $S$. Therefore $\tilde{x}$ is the maximal element in $S$, thus $f=0$ in some ball centered at $\tilde{x}$. We picked $\tilde{x}$ to be arbitrary, so $f=0$ everywhere in $(a,b)$.

\corollary{
    Let $x_0\in (a,b)$, and $f$ is analytic on $(a,b)$, $f$ equals the power series centered at $x_0$ where the power series converges.
}
\begin{proof}
    The power series is analytic, and equals $f$ on some small open ball in $(a,b)$.
\end{proof}
\exercises
\begin{exerciselist}
    \item Find the Taylor Series for the given functions at the indicated points. \begin{enumerate}[label=(\alph*)]
        \item $f(x) = e^{-x}, x_0 = 0.$
        \item $f(x) = e^x, x_0 = 1.$ 
        \item $f(x)=1/x, x_0 = 1.$
        \item $f(x) = \cos(x), x_0 = \pi/2.$ 
        \item $f(x) = \ln(x), x_0 = 1.$
    \end{enumerate}
    \item Determine the radius of convergence of the given function about $x=0$. \begin{enumerate}[label=(\alph*)]
        \item $f(x) = (1+x)/(x-2).$
        \item $f(x) = 2x/(1+2x^2).$
        \item $f(x) = 1/(1-t^3).$
        \item $f(x) = ((t-4)(t^2+3))^{-1}.$
    \end{enumerate} 
\end{exerciselist}

\section{Taylor's Theorem with Remainder}
Sometimes we don't want to take the whole power series representation, but truncate the series to get an approximation for the functions.

\todo show that the n-th order taylor series is the best n-th order approximation for a function.
As Taylor Series are used to approximate functions, it is of relevance to determine the accuracy of a series in representing its desired function. 
\definition{Taylor's Formula with Remainder}{The remainder of order n of the Taylor expansion of $f(x_o)$ is represented by the function,
\[R_n(x) = f(x) - \sum_{k=0}^{n-1} \frac{f^{(k)}(x_o)}{k!}(x-x_o)^k.\]
}
The remainder of a Taylor expansion is the difference between the value of a function $f$ at $x$ and the partial sum of the $n^{th}$ term Taylor series. The series converges if $\lim_{n \rightarrow \infty} R_n = 0$. 

\theorem{}{Let $f(x)$ be a function on the interval $(a,b)$. $f$ is analytic on $(a,b)$ if there exists and $M>0$ such that 
\[|f^{(n)}(x) \leq M^n\]
for all $x \in (a,b)$ and $n \in \mathbb{N}$. 
}
As a result of this theorem, the Taylor series expansion holds for all $x \in (a,b)$.
\proof{Let $f(x)$ be a function on the interval $(a,b)$ and $x_o \in (a,b)$. For some $M \in \reals$ set $C = max{M|a-x_o, M|b-x_o|}$. Then the $n^{th}$ term remainder of the Taylor expansion of $f(x)$ at $x_o$ is given by
\[R_n = f(x) - \sum_{k=0}^{n-1}\frac{f^{(k)}(x_o)}{k!}(x-x_o)^k = \sum_{k=n}^{\infty} \frac{f^{(k)}(x_o)}{k!}(x-x_o)^k \]
Each term in this infinite series for $R_n$ is given by
\[R_k = \frac{f^{(k)}(x_o)}{k!}(x-x_o)^k \leq \frac{M^k}{k!}\]}

\section{Multidimesnional Taylor Series}
We look to extend the formulation for the Taylor series to approximate functions of multiple variables. 
\definition{}

\section{Extrema of Multivariate Functions}
Just as we are interested in finding the extreme points of functions of a single variable, we likewise wish to solve for stationary points of multivariate functions. Analysis of functions of multiple variables have analogous first and second derivative tests to those learned in single variable calculus. I used the term 'stationary point' as in three dimensions in addition to maxima or minima there exist so called 
saddle points. A 3D representation of a saddle point is presented in figure 
\definition{Multivariable First Derivative Test} {A point $(x_o, y_o) \in \mathbb{R^2}$ is a \textit{stationary point} of some function $f(x,y)$ if
\[\nabla f \rvert_{(x, y) = (x_o, y_o)} = 0\]}
Also similarly to the second derivative test in single variable calculus, we also have an analogous second derivative test in multivariable calculus to determine the classification of critical points. For this we use the discussion of multivariable Taylor series discussed in section . To second order, the Taylor expansion of some function $f(x,y)$ around $(x_o, y_o)$ is
\[f(x,y) \approx f(x_o, y_o) + \nabla f(x_o, y_o)^T d + \frac{1}{2!}d^THf(x_o,y_o)d + R_2(x,y)\]
From the first derivative test $\nabla f(x_o, y_o) = 0$ thereby eliminating that term. Also, we know $R_2 \rightarrow 0, (x,y) \rightarrow (x_o, y_o)$. Thus we have
\[f(x,y) \approx f(x_o, y_o) + \frac{1}{2!}d^THf(x_o,y_o)d \]
Rearranging we have 
\[f(x,y) - f(x_o, y_o) = \frac{1}{2!}d^THf(x_o,y_o)d \]
The left side appears as the numerator of the definition of the derivative. The sign of our derivative is dependent on the Hessian matrix $H$ which holds for all points $(x,y)$ near $(x_o, y_o)$. 
\section{Lagrange Multipliers}

